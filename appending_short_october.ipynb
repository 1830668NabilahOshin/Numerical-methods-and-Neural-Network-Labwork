{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1830668NabilahOshin/Numerical-methods-and-Neural-Network-Labwork/blob/main/appending_short_october.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uFjmyRGot8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785891da-cb91-4302-e23e-c98ed3696ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from gspread) (2.31.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from googleapiclient.discovery import build\n",
        "!pip install gspread\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dzlWJnDapxAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5t_jPjXp353",
        "outputId": "05e687af-7b42-4a8a-ce31-229c5068f21c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEiGoBPgqtAm",
        "outputId": "0f06c860-4a70-4943-bed4-1edeb082c6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/981.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.9/981.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=7ad350259da8a970be822b16f18c014127326fe84104ea02066ba106cbffe026\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n"
      ],
      "metadata": {
        "id": "CUfA8NtfqCCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"*****************\"\n",
        "\n",
        "def extract_comments(video_id):\n",
        "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "\n",
        "    video_response = youtube.videos().list(part='snippet', id=video_id).execute()\n",
        "    video_title = video_response['items'][0]['snippet']['title']\n",
        "\n",
        "    comments = []\n",
        "    next_page_token = None\n",
        "    existing_comments = set()\n",
        "\n",
        "    while True:\n",
        "        comments_response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            pageToken=next_page_token,\n",
        "            maxResults=200,\n",
        "            order='relevance'\n",
        "        ).execute()\n",
        "\n",
        "        for comment in comments_response['items']:\n",
        "            comment_text = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            if is_bengali(comment_text) and is_long_comment(comment_text, 4) and comment_text not in existing_comments:\n",
        "                comments.append(comment_text)\n",
        "                existing_comments.add(comment_text)\n",
        "                if len(comments) == 60:\n",
        "                    return video_title, comments\n",
        "\n",
        "        next_page_token = comments_response.get('nextPageToken')\n",
        "\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return video_title, comments\n",
        "\n",
        "#def is_bengali(text):\n",
        "    #bengali_pattern = re.compile(r'^[\\u0980-\\u09FF\\s]+$')\n",
        "    #return bool(bengali_pattern.match(text))\n",
        "\n",
        "def is_bengali(text):\n",
        "    # Update the regular expression pattern to include a wider range of characters\n",
        "    bengali_pattern = re.compile(r'^[a-zA-Z0-9\\u0980-\\u09FF\\s.,!?😀-😮]+$')\n",
        "    return bool(bengali_pattern.match(text))\n",
        "\n",
        "def is_long_comment(text, min_words):\n",
        "    words = text.split()\n",
        "    return len(words) > min_words\n",
        "\n",
        "\n",
        "video_ids2 = [\"MfdIW-gFk4w\", \"Fl1RKHdU-IA\",\"emC-PO20t-k\",\"QoXfL33QWbc\",\"JIdFx3uJC6E\",\"-tLHhA-TH6I\",\"gXswrTUWZHc\"]\n",
        "video_ids3 = []\n",
        "#commas = [\"\", \"\",]\n",
        "\n"
      ],
      "metadata": {
        "id": "sf2wG0pit7Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_bengali(text):\n",
        "    try:\n",
        "        language = detect(text)\n",
        "        if language == \"bn\":\n",
        "            # Check if the comment contains emojis, punctuations, or special characters\n",
        "            if any(char in text for char in \".,!?😀-😮\"):\n",
        "                return True\n",
        "        return False\n",
        "    except:\n",
        "        return False"
      ],
      "metadata": {
        "id": "N6jxt957o3iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate and open the Google Sheets file\n",
        "credentials_path = \"/content/drive/MyDrive/moo-high-men/banglaytcomments-73184086a64a.json\"\n",
        "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_path, scope)\n",
        "client = gspread.authorize(credentials)\n",
        "\n",
        "# Specify the Google Sheets file name or URL\n",
        "gsheet_file = \"Bangla Error Corrector Dataset from YouTube Comments\"\n",
        "\n",
        "# Open the Google Sheets file\n",
        "gsheet = client.open(gsheet_file)\n",
        "\n",
        "# Select the first sheet in the Google Sheets file\n",
        "worksheet = gsheet.get_worksheet(8)\n",
        "\n",
        "# Get all existing comments from the worksheet\n",
        "existing_comments = set(worksheet.col_values(2)[1:])\n",
        "\n"
      ],
      "metadata": {
        "id": "Ws3_byzTuCAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3ffLHsjo_aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ... (previous code)\n",
        "\n",
        "# Create an empty DataFrame to store video title and comments\n",
        "comments_df = pd.DataFrame(columns=['Video Title', 'Comment'])\n",
        "\n",
        "rows_data = []\n",
        "for video_id in video_ids3:\n",
        "    video_title, comments = extract_comments(video_id)\n",
        "    for comment in comments:\n",
        "        if comment not in existing_comments:\n",
        "            comments_df = comments_df.append({'Video Title': video_title, 'Comment': comment}, ignore_index=True)\n",
        "            rows_data.append([video_title, comment])\n",
        "            existing_comments.add(comment)\n",
        "\n",
        "\n",
        "\n",
        "worksheet.append_rows(rows_data)\n",
        "\n",
        "# Get the URL of the Google Sheets file\n",
        "gsheet_url = gsheet.url\n",
        "\n",
        "# Print the URL\n",
        "print(f\"Google Sheets file URL: {gsheet_url}\")\n",
        "\n",
        "# Print the path to the Google Sheets file\n",
        "print(f\"Bengali comments appended to {gsheet_file}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-GCnglsyuTQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(comments_df.shape)"
      ],
      "metadata": {
        "id": "PceqWVUEBTSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6cc41a-8fe8-4787-d46b-145badde1498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ========================================================================="
      ],
      "metadata": {
        "id": "ZxzTvH5Twi8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDQG5ra_wx8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(comments_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHaKggqQxHWV",
        "outputId": "28defdfe-b081-4ad6-ab13-ead3b176c28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 5)\n"
          ]
        }
      ]
    }
  ]
}